<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kubernetes 基于 Namespace 的物理队列实现 | 九原山</title><meta name=keywords content="blog"><meta name=description content="Kubernetes 基于 Namespace 的物理队列实现 作者：swulling+pub@gmail.com
摘要：Kubernetes 实现基于 Namespace 的物理队列，即Namespace下的Pod和Node的强绑定
0x00 背景 Kuberntes 目前在实际业务部署时，有两个流派：一派推崇小集群，一个或数个业务共享小集群，全公司有数百上千个小集群组成；另一派推崇大集群，每个AZ（可用区）一个或数个大集群，各个业务通过Namespace的方式进行隔离。
两者各有优劣，但是从资源利用率提升和维护成本的角度，大集群的优势更加突出。但同时大集群也带来相当多的安全、可用性、性能的挑战和维护管理成本。
本文属于Kubernetes多租户大集群实践的一部分，用来解决多租户场景下，如何实现传统的物理队列隔离。
物理队列并不是一个通用的业界名词，它来源于一种集群资源管理模型，该模型简化下如下：
逻辑队列（Logical Queue）：逻辑队列是虚拟资源分配的最小单元，将虚拟资源配额（Quota）配置在逻辑队列上（如CPU 200 标准核、内存 800GB等） 逻辑队列对应Kubernetes的Namespace概念。参考Resource Quotas 不同的逻辑队列之间可以设置Qos优先级，实现优先级调度。参考Limit Priority Class consumption by default可以限制每个Namespace下Pod的优先级选择 配额分两种：Requests（提供保障的资源）和Limits（资源的最大限制），其中仅Requests才能算Quota，Limits 由管理员视情况选择 物理队列（Physical Queue）：物理队列对应底层物理机资源，同一台物理机仅能从属于同一个物理队列。物理队列的资源总额就是其下物理机可提供的资源的总和。 物理队列当前在Kubernetes下缺乏概念映射 逻辑队列和物理队列是多对多绑定的关系，即同一个逻辑队列可以跨多个物理队列。 逻辑队列的配额总和 / 物理队列的资源总和 = 全局超售比 租户：租户可以绑定多个逻辑队列，对应关系仅影响往对应的Namespace中部署Pod的权限。 资源结构如图所示：
0x01 原理 物理队列实现：
给节点配置Label和Taint，Label用于选择，Taint用于拒绝非该物理队列的Pod部署。 和Namespace的自动绑定的原理：
配置两个Admission Controller: PodNodeSelector和PodTolerationRestriction，参考Admission Controllers 给Namespace增加默认的NodeSelector和Tolerations策略，并自动应用到该 Namespace 下的全部新增 Pod 上，从而自动将Pod绑定到物理队列上。 0x02 配置 测试集群版本：1.16.4, 1.17.0，使用的测试集群为Kind创建 1.18.0 版本的Kind集群创建有问题，后续进行测试
# this config file contains all config fields with comments # NOTE: this is not a particularly useful config file kind: Cluster apiVersion: kind."><meta name=author content><link rel=canonical href=https://ninehills.tech/posts/ninehills-ninehills.github.io-597686461-post-77/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c88963fe2d79462000fd0fb1b3737783c32855d340583e4523343f8735c787f0.css integrity="sha256-yIlj/i15RiAA/Q+xs3N3g8MoVdNAWD5FIzQ/hzXHh/A=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://ninehills.tech/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ninehills.tech/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ninehills.tech/favicon-32x32.png><link rel=apple-touch-icon href=https://ninehills.tech/apple-touch-icon.png><link rel=mask-icon href=https://ninehills.tech/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.112.6"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Kubernetes 基于 Namespace 的物理队列实现"><meta property="og:description" content="Kubernetes 基于 Namespace 的物理队列实现 作者：swulling+pub@gmail.com
摘要：Kubernetes 实现基于 Namespace 的物理队列，即Namespace下的Pod和Node的强绑定
0x00 背景 Kuberntes 目前在实际业务部署时，有两个流派：一派推崇小集群，一个或数个业务共享小集群，全公司有数百上千个小集群组成；另一派推崇大集群，每个AZ（可用区）一个或数个大集群，各个业务通过Namespace的方式进行隔离。
两者各有优劣，但是从资源利用率提升和维护成本的角度，大集群的优势更加突出。但同时大集群也带来相当多的安全、可用性、性能的挑战和维护管理成本。
本文属于Kubernetes多租户大集群实践的一部分，用来解决多租户场景下，如何实现传统的物理队列隔离。
物理队列并不是一个通用的业界名词，它来源于一种集群资源管理模型，该模型简化下如下：
逻辑队列（Logical Queue）：逻辑队列是虚拟资源分配的最小单元，将虚拟资源配额（Quota）配置在逻辑队列上（如CPU 200 标准核、内存 800GB等） 逻辑队列对应Kubernetes的Namespace概念。参考Resource Quotas 不同的逻辑队列之间可以设置Qos优先级，实现优先级调度。参考Limit Priority Class consumption by default可以限制每个Namespace下Pod的优先级选择 配额分两种：Requests（提供保障的资源）和Limits（资源的最大限制），其中仅Requests才能算Quota，Limits 由管理员视情况选择 物理队列（Physical Queue）：物理队列对应底层物理机资源，同一台物理机仅能从属于同一个物理队列。物理队列的资源总额就是其下物理机可提供的资源的总和。 物理队列当前在Kubernetes下缺乏概念映射 逻辑队列和物理队列是多对多绑定的关系，即同一个逻辑队列可以跨多个物理队列。 逻辑队列的配额总和 / 物理队列的资源总和 = 全局超售比 租户：租户可以绑定多个逻辑队列，对应关系仅影响往对应的Namespace中部署Pod的权限。 资源结构如图所示：
0x01 原理 物理队列实现：
给节点配置Label和Taint，Label用于选择，Taint用于拒绝非该物理队列的Pod部署。 和Namespace的自动绑定的原理：
配置两个Admission Controller: PodNodeSelector和PodTolerationRestriction，参考Admission Controllers 给Namespace增加默认的NodeSelector和Tolerations策略，并自动应用到该 Namespace 下的全部新增 Pod 上，从而自动将Pod绑定到物理队列上。 0x02 配置 测试集群版本：1.16.4, 1.17.0，使用的测试集群为Kind创建 1.18.0 版本的Kind集群创建有问题，后续进行测试
# this config file contains all config fields with comments # NOTE: this is not a particularly useful config file kind: Cluster apiVersion: kind."><meta property="og:type" content="article"><meta property="og:url" content="https://ninehills.tech/posts/ninehills-ninehills.github.io-597686461-post-77/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-04-10T04:22:00+00:00"><meta property="article:modified_time" content="2020-04-10T04:22:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes 基于 Namespace 的物理队列实现"><meta name=twitter:description content="Kubernetes 基于 Namespace 的物理队列实现 作者：swulling+pub@gmail.com
摘要：Kubernetes 实现基于 Namespace 的物理队列，即Namespace下的Pod和Node的强绑定
0x00 背景 Kuberntes 目前在实际业务部署时，有两个流派：一派推崇小集群，一个或数个业务共享小集群，全公司有数百上千个小集群组成；另一派推崇大集群，每个AZ（可用区）一个或数个大集群，各个业务通过Namespace的方式进行隔离。
两者各有优劣，但是从资源利用率提升和维护成本的角度，大集群的优势更加突出。但同时大集群也带来相当多的安全、可用性、性能的挑战和维护管理成本。
本文属于Kubernetes多租户大集群实践的一部分，用来解决多租户场景下，如何实现传统的物理队列隔离。
物理队列并不是一个通用的业界名词，它来源于一种集群资源管理模型，该模型简化下如下：
逻辑队列（Logical Queue）：逻辑队列是虚拟资源分配的最小单元，将虚拟资源配额（Quota）配置在逻辑队列上（如CPU 200 标准核、内存 800GB等） 逻辑队列对应Kubernetes的Namespace概念。参考Resource Quotas 不同的逻辑队列之间可以设置Qos优先级，实现优先级调度。参考Limit Priority Class consumption by default可以限制每个Namespace下Pod的优先级选择 配额分两种：Requests（提供保障的资源）和Limits（资源的最大限制），其中仅Requests才能算Quota，Limits 由管理员视情况选择 物理队列（Physical Queue）：物理队列对应底层物理机资源，同一台物理机仅能从属于同一个物理队列。物理队列的资源总额就是其下物理机可提供的资源的总和。 物理队列当前在Kubernetes下缺乏概念映射 逻辑队列和物理队列是多对多绑定的关系，即同一个逻辑队列可以跨多个物理队列。 逻辑队列的配额总和 / 物理队列的资源总和 = 全局超售比 租户：租户可以绑定多个逻辑队列，对应关系仅影响往对应的Namespace中部署Pod的权限。 资源结构如图所示：
0x01 原理 物理队列实现：
给节点配置Label和Taint，Label用于选择，Taint用于拒绝非该物理队列的Pod部署。 和Namespace的自动绑定的原理：
配置两个Admission Controller: PodNodeSelector和PodTolerationRestriction，参考Admission Controllers 给Namespace增加默认的NodeSelector和Tolerations策略，并自动应用到该 Namespace 下的全部新增 Pod 上，从而自动将Pod绑定到物理队列上。 0x02 配置 测试集群版本：1.16.4, 1.17.0，使用的测试集群为Kind创建 1.18.0 版本的Kind集群创建有问题，后续进行测试
# this config file contains all config fields with comments # NOTE: this is not a particularly useful config file kind: Cluster apiVersion: kind."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://ninehills.tech/posts/"},{"@type":"ListItem","position":2,"name":"Kubernetes 基于 Namespace 的物理队列实现","item":"https://ninehills.tech/posts/ninehills-ninehills.github.io-597686461-post-77/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes 基于 Namespace 的物理队列实现","name":"Kubernetes 基于 Namespace 的物理队列实现","description":"Kubernetes 基于 Namespace 的物理队列实现 作者：swulling+pub@gmail.com\n摘要：Kubernetes 实现基于 Namespace 的物理队列，即Namespace下的Pod和Node的强绑定\n0x00 背景 Kuberntes 目前在实际业务部署时，有两个流派：一派推崇小集群，一个或数个业务共享小集群，全公司有数百上千个小集群组成；另一派推崇大集群，每个AZ（可用区）一个或数个大集群，各个业务通过Namespace的方式进行隔离。\n两者各有优劣，但是从资源利用率提升和维护成本的角度，大集群的优势更加突出。但同时大集群也带来相当多的安全、可用性、性能的挑战和维护管理成本。\n本文属于Kubernetes多租户大集群实践的一部分，用来解决多租户场景下，如何实现传统的物理队列隔离。\n物理队列并不是一个通用的业界名词，它来源于一种集群资源管理模型，该模型简化下如下：\n逻辑队列（Logical Queue）：逻辑队列是虚拟资源分配的最小单元，将虚拟资源配额（Quota）配置在逻辑队列上（如CPU 200 标准核、内存 800GB等） 逻辑队列对应Kubernetes的Namespace概念。参考Resource Quotas 不同的逻辑队列之间可以设置Qos优先级，实现优先级调度。参考Limit Priority Class consumption by default可以限制每个Namespace下Pod的优先级选择 配额分两种：Requests（提供保障的资源）和Limits（资源的最大限制），其中仅Requests才能算Quota，Limits 由管理员视情况选择 物理队列（Physical Queue）：物理队列对应底层物理机资源，同一台物理机仅能从属于同一个物理队列。物理队列的资源总额就是其下物理机可提供的资源的总和。 物理队列当前在Kubernetes下缺乏概念映射 逻辑队列和物理队列是多对多绑定的关系，即同一个逻辑队列可以跨多个物理队列。 逻辑队列的配额总和 / 物理队列的资源总和 = 全局超售比 租户：租户可以绑定多个逻辑队列，对应关系仅影响往对应的Namespace中部署Pod的权限。 资源结构如图所示：\n0x01 原理 物理队列实现：\n给节点配置Label和Taint，Label用于选择，Taint用于拒绝非该物理队列的Pod部署。 和Namespace的自动绑定的原理：\n配置两个Admission Controller: PodNodeSelector和PodTolerationRestriction，参考Admission Controllers 给Namespace增加默认的NodeSelector和Tolerations策略，并自动应用到该 Namespace 下的全部新增 Pod 上，从而自动将Pod绑定到物理队列上。 0x02 配置 测试集群版本：1.16.4, 1.17.0，使用的测试集群为Kind创建 1.18.0 版本的Kind集群创建有问题，后续进行测试\n# this config file contains all config fields with comments # NOTE: this is not a particularly useful config file kind: Cluster apiVersion: kind.","keywords":["blog"],"articleBody":"Kubernetes 基于 Namespace 的物理队列实现 作者：swulling+pub@gmail.com\n摘要：Kubernetes 实现基于 Namespace 的物理队列，即Namespace下的Pod和Node的强绑定\n0x00 背景 Kuberntes 目前在实际业务部署时，有两个流派：一派推崇小集群，一个或数个业务共享小集群，全公司有数百上千个小集群组成；另一派推崇大集群，每个AZ（可用区）一个或数个大集群，各个业务通过Namespace的方式进行隔离。\n两者各有优劣，但是从资源利用率提升和维护成本的角度，大集群的优势更加突出。但同时大集群也带来相当多的安全、可用性、性能的挑战和维护管理成本。\n本文属于Kubernetes多租户大集群实践的一部分，用来解决多租户场景下，如何实现传统的物理队列隔离。\n物理队列并不是一个通用的业界名词，它来源于一种集群资源管理模型，该模型简化下如下：\n逻辑队列（Logical Queue）：逻辑队列是虚拟资源分配的最小单元，将虚拟资源配额（Quota）配置在逻辑队列上（如CPU 200 标准核、内存 800GB等） 逻辑队列对应Kubernetes的Namespace概念。参考Resource Quotas 不同的逻辑队列之间可以设置Qos优先级，实现优先级调度。参考Limit Priority Class consumption by default可以限制每个Namespace下Pod的优先级选择 配额分两种：Requests（提供保障的资源）和Limits（资源的最大限制），其中仅Requests才能算Quota，Limits 由管理员视情况选择 物理队列（Physical Queue）：物理队列对应底层物理机资源，同一台物理机仅能从属于同一个物理队列。物理队列的资源总额就是其下物理机可提供的资源的总和。 物理队列当前在Kubernetes下缺乏概念映射 逻辑队列和物理队列是多对多绑定的关系，即同一个逻辑队列可以跨多个物理队列。 逻辑队列的配额总和 / 物理队列的资源总和 = 全局超售比 租户：租户可以绑定多个逻辑队列，对应关系仅影响往对应的Namespace中部署Pod的权限。 资源结构如图所示：\n0x01 原理 物理队列实现：\n给节点配置Label和Taint，Label用于选择，Taint用于拒绝非该物理队列的Pod部署。 和Namespace的自动绑定的原理：\n配置两个Admission Controller: PodNodeSelector和PodTolerationRestriction，参考Admission Controllers 给Namespace增加默认的NodeSelector和Tolerations策略，并自动应用到该 Namespace 下的全部新增 Pod 上，从而自动将Pod绑定到物理队列上。 0x02 配置 测试集群版本：1.16.4, 1.17.0，使用的测试集群为Kind创建 1.18.0 版本的Kind集群创建有问题，后续进行测试\n# this config file contains all config fields with comments # NOTE: this is not a particularly useful config file kind: Cluster apiVersion: kind.x-k8s.io/v1alpha4 # patch the generated kubeadm config with some extra settings kubeadmConfigPatches: - | apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration evictionHard: nodefs.available: \"0%\" - | apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration apiServer: extraArgs: enable-admission-plugins: PodNodeSelector,PodTolerationRestriction # 1 control plane node and 3 workers nodes: # the control plane node config - role: control-plane # the three workers - role: worker - role: worker - role: worker 集群开启Admission Controller: PodNodeSelector,PodTolerationRestriction 可以使用 kubeadmin 或 api-server启动参数：\napiServer: extraArgs: enable-admission-plugins: PodNodeSelector,PodTolerationRestriction 创建 Namespace apiVersion: v1 kind: Namespace metadata: name: public annotations: scheduler.alpha.kubernetes.io/node-selector: \"node-restriction.kubernetes.io/physical_queue=public-phy\" scheduler.alpha.kubernetes.io/defaultTolerations: '[{\"operator\": \"Equal\", \"effect\": \"NoSchedule\", \"key\": \"node-restriction.kubernetes.io/physical_queue\", \"value\": \"public-phy\"}]' # scheduler.alpha.kubernetes.io/tolerationsWhitelist: '[{\"operator\": \"Equal\", \"effect\": \"NoSchedule\", \"key\": \"node-restriction.kubernetes.io/physical_queue\", \"value\": \"public-phy\"}]' 此处要点：\n文档有问题，toleration 配置是一个list，配置错误在部署时会提示解析JSON错误 tolerationsWhitelist配置后，就算配置有defaultTolerations且相同，也需要在Pod中指定对应的toleration，所以不能配置 NoSchedule 已经足够限制，无需 NoExecute，Node配置的时候同样配置，此处可根据需求进行选择。 物理队列的前缀建议为 node-restriction.kubernetes.io/physical_queue，此处是根据文档的建议，后续可以配合NodeRestriction admission plugin限制kubelet自定配置 目前Namespace尚不能绑定多个物理队列: NodeSelector 无法支持in语法，见 0x04 defaultTolerations 可以配置多个 Torleration 给Node绑定物理队列 $ kubectl label node kind-worker node-restriction.kubernetes.io/physical_queue=public-phy $ kubectl taint nodes kind-worker node-restriction.kubernetes.io/physical_queue=public-phy:NoSchedule 0x03 测试 测试的Deployment如下 apiVersion: apps/v1\rkind: Deployment\rmetadata:\rname: nginx-deployment\rlabels:\rapp: nginx\rspec:\rreplicas: 3\rselector:\rmatchLabels:\rapp: nginx\rtemplate:\rmetadata:\rlabels:\rapp: nginx\rspec:\rcontainers:\r- name: nginx\rimage: nginx:1.14.2\rports:\r- containerPort: 80 验证提交到指定物理队列中的Pod默认增加NodeSelector和Toleration $ kubectl apply -f nginx_deployment.yaml --namespace public $ kubectl describe pod nginx-deployment-574b87c764-kb9k7 --namespace public Node-Selectors: node-restriction.kubernetes.io/physical_queue=public-phy Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s node-restriction.kubernetes.io/physical_queue=public-phy:NoSchedule 验证Pod是否都配置到一起 kubectl describe node kind-worker 验证和物理队列中指定的NodeSelector冲突的Pod无法提交 $ kubectl delete deployment nginx-deployment --namespace public # 修改nginx_deployment.yaml ，增加spec.template.spec.nodeSelector nodeSelector: node-restriction.kubernetes.io/physical_queue: second-phy # 验证能否部署 $ kubectl apply -f nginx_deployment.yaml --namespace public # 查看deployments $ kubectl describe replicaset nginx-deployment-585fcd8d7d --namespace public Warning FailedCreate 49s (x15 over 2m11s) replicaset-controller Error creating: pods is forbidden: pod node label selector conflicts with its namespace node label selector 0x04 相关问题 Q: NodeSelector无法使用Set-based语法，导致逻辑队列（NameSpace）无法绑定多个物理队列 后续考虑使用Node Affinity配置节点亲和性。但是目前并没有现成的Adminssion Controller去给Namespace绑定默认的节点亲和性，如有需求需要自己开发。\nNodeSelector 和 Toleration 的功能，可以被 Node Affinity 进行替代，且后者提供更高级的调度功能，后续尝试是否基于此进行资源调度的整体设计。\n此外Node Affinity还可以实现一个逻辑队列绑定多个物理队列的情况下，配置物理队列的调度权重的功能，即优先部署到某个物理队列。\n","wordCount":"357","inLanguage":"en","datePublished":"2020-04-10T04:22:00Z","dateModified":"2020-04-10T04:22:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://ninehills.tech/posts/ninehills-ninehills.github.io-597686461-post-77/"},"publisher":{"@type":"Organization","name":"九原山","logo":{"@type":"ImageObject","url":"https://ninehills.tech/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ninehills.tech/ accesskey=h title="九原山 (Alt + H)">九原山</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Kubernetes 基于 Namespace 的物理队列实现</h1><div class=post-meta><span title='2020-04-10 04:22:00 +0000 UTC'>April 10, 2020</span></div></header><div class=post-content><h1 id=kubernetes-基于-namespace-的物理队列实现>Kubernetes 基于 Namespace 的物理队列实现<a hidden class=anchor aria-hidden=true href=#kubernetes-基于-namespace-的物理队列实现>#</a></h1><blockquote><p>作者：<code>swulling+pub@gmail.com</code><br>摘要：Kubernetes 实现基于 Namespace 的物理队列，即Namespace下的Pod和Node的强绑定</p></blockquote><h2 id=0x00-背景>0x00 背景<a hidden class=anchor aria-hidden=true href=#0x00-背景>#</a></h2><p>Kuberntes 目前在实际业务部署时，有两个流派：一派推崇小集群，一个或数个业务共享小集群，全公司有数百上千个小集群组成；另一派推崇大集群，每个AZ（可用区）一个或数个大集群，各个业务通过Namespace的方式进行隔离。</p><p>两者各有优劣，但是从资源利用率提升和维护成本的角度，大集群的优势更加突出。但同时大集群也带来相当多的安全、可用性、性能的挑战和维护管理成本。</p><p>本文属于Kubernetes多租户大集群实践的一部分，用来解决多租户场景下，如何实现传统的物理队列隔离。</p><p>物理队列并不是一个通用的业界名词，它来源于一种集群资源管理模型，该模型简化下如下：</p><ul><li><strong>逻辑队列（Logical Queue）</strong>：逻辑队列是虚拟资源分配的最小单元，将虚拟资源配额（Quota）配置在逻辑队列上（如CPU 200 标准核、内存 800GB等）<ul><li>逻辑队列<strong>对应Kubernetes的Namespace概念</strong>。参考<a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/>Resource Quotas</a></li><li>不同的逻辑队列之间可以设置Qos优先级，实现优先级调度。参考<a href=https://kubernetes.io/docs/concepts/policy/resource-quotas/#limit-priority-class-consumption-by-default>Limit Priority Class consumption by default</a>可以限制每个Namespace下Pod的优先级选择</li><li>配额分两种：Requests（提供保障的资源）和Limits（资源的最大限制），其中仅Requests才能算Quota，Limits 由管理员视情况选择</li></ul></li><li><strong>物理队列（Physical Queue）</strong>：物理队列对应底层物理机资源，同一台物理机仅能从属于同一个物理队列。物理队列的资源总额就是其下物理机可提供的资源的总和。<ul><li>物理队列<strong>当前在Kubernetes下缺乏概念映射</strong></li><li>逻辑队列和物理队列是多对多绑定的关系，即同一个逻辑队列可以跨多个物理队列。</li><li>逻辑队列的配额总和 / 物理队列的资源总和 = 全局超售比</li></ul></li><li><strong>租户</strong>：租户可以绑定多个逻辑队列，对应关系仅影响往对应的Namespace中部署Pod的权限。</li></ul><p>资源结构如图所示：</p><p><img loading=lazy src=https://user-images.githubusercontent.com/270298/81274914-8681f200-9083-11ea-8012-a70ef5ce414d.png alt=物理队列和逻辑队列></p><h2 id=0x01-原理>0x01 原理<a hidden class=anchor aria-hidden=true href=#0x01-原理>#</a></h2><p>物理队列实现：</p><ul><li>给节点配置Label和Taint，Label用于选择，Taint用于拒绝非该物理队列的Pod部署。</li></ul><p>和<code>Namespace</code>的自动绑定的原理：</p><ul><li>配置两个<code>Admission Controller</code>: <code>PodNodeSelector</code>和<code>PodTolerationRestriction</code>，参考<a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podtolerationrestriction>Admission Controllers</a></li><li>给<code>Namespace</code>增加默认的<code>NodeSelector</code>和<code>Tolerations</code>策略，并自动应用到该 Namespace 下的全部新增 Pod 上，从而自动将Pod绑定到物理队列上。</li></ul><h2 id=0x02-配置>0x02 配置<a hidden class=anchor aria-hidden=true href=#0x02-配置>#</a></h2><ol><li>测试集群版本：1.16.4, 1.17.0，使用的测试集群为<a href=https://kind.sigs.k8s.io/>Kind</a>创建</li></ol><blockquote><p>1.18.0 版本的Kind集群创建有问题，后续进行测试</p></blockquote><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># this config file contains all config fields with comments</span>
</span></span><span style=display:flex><span><span style=color:#75715e># NOTE: this is not a particularly useful config file</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Cluster</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>kind.x-k8s.io/v1alpha4</span>
</span></span><span style=display:flex><span><span style=color:#75715e># patch the generated kubeadm config with some extra settings</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kubeadmConfigPatches</span>:
</span></span><span style=display:flex><span>- |<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  apiVersion: kubelet.config.k8s.io/v1beta1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  kind: KubeletConfiguration
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  evictionHard:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    nodefs.available: &#34;0%&#34;</span>  
</span></span><span style=display:flex><span>- |<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  apiVersion: kubeadm.k8s.io/v1beta2
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  kind: ClusterConfiguration
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  apiServer:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    extraArgs:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>      enable-admission-plugins: PodNodeSelector,PodTolerationRestriction</span>  
</span></span><span style=display:flex><span><span style=color:#75715e># 1 control plane node and 3 workers</span>
</span></span><span style=display:flex><span><span style=color:#f92672>nodes</span>:
</span></span><span style=display:flex><span><span style=color:#75715e># the control plane node config</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>role</span>: <span style=color:#ae81ff>control-plane</span>
</span></span><span style=display:flex><span><span style=color:#75715e># the three workers</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>role</span>: <span style=color:#ae81ff>worker</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>role</span>: <span style=color:#ae81ff>worker</span>
</span></span><span style=display:flex><span>- <span style=color:#f92672>role</span>: <span style=color:#ae81ff>worker</span>
</span></span></code></pre></div><ol start=2><li>集群开启<a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#podtolerationrestriction>Admission Controller</a>: PodNodeSelector,PodTolerationRestriction</li></ol><p>可以使用 <a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/#apiserver-flags>kubeadmin</a> 或 api-server启动参数：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiServer</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>  extraArgs</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>    enable-admission-plugins</span>: <span style=color:#ae81ff>PodNodeSelector,PodTolerationRestriction</span>
</span></span></code></pre></div><ol start=3><li>创建 Namespace</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Namespace</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>  name</span>: <span style=color:#ae81ff>public</span>
</span></span><span style=display:flex><span><span style=color:#f92672>  annotations</span>:
</span></span><span style=display:flex><span><span style=color:#f92672>    scheduler.alpha.kubernetes.io/node-selector</span>: <span style=color:#e6db74>&#34;node-restriction.kubernetes.io/physical_queue=public-phy&#34;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>    scheduler.alpha.kubernetes.io/defaultTolerations</span>: <span style=color:#e6db74>&#39;[{&#34;operator&#34;: &#34;Equal&#34;, &#34;effect&#34;: &#34;NoSchedule&#34;, &#34;key&#34;: &#34;node-restriction.kubernetes.io/physical_queue&#34;, &#34;value&#34;: &#34;public-phy&#34;}]&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>    # scheduler.alpha.kubernetes.io/tolerationsWhitelist</span>: <span style=color:#e6db74>&#39;[{&#34;operator&#34;: &#34;Equal&#34;, &#34;effect&#34;: &#34;NoSchedule&#34;, &#34;key&#34;: &#34;node-restriction.kubernetes.io/physical_queue&#34;, &#34;value&#34;: &#34;public-phy&#34;}]&#39;</span>
</span></span></code></pre></div><p>此处要点：</p><ul><li>文档有问题，toleration 配置是一个list，配置错误在部署时会提示解析JSON错误</li><li>tolerationsWhitelist配置后，就算配置有defaultTolerations且相同，也需要在Pod中指定对应的toleration，所以不能配置</li><li>NoSchedule 已经足够限制，无需 NoExecute，Node配置的时候同样配置，此处可根据需求进行选择。</li><li>物理队列的前缀建议为 <code>node-restriction.kubernetes.io/physical_queue</code>，此处是根据<a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-isolation-restriction>文档</a>的建议，后续可以配合<a href=https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction>NodeRestriction admission plugin</a>限制kubelet自定配置</li><li>目前Namespace尚不能绑定多个物理队列:<ul><li>NodeSelector 无法支持in语法，见 0x04</li><li>defaultTolerations 可以配置多个 Torleration</li></ul></li></ul><ol start=4><li>给Node绑定物理队列</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl label node kind-worker node-restriction.kubernetes.io/physical_queue<span style=color:#f92672>=</span>public-phy
</span></span><span style=display:flex><span>$ kubectl taint nodes kind-worker node-restriction.kubernetes.io/physical_queue<span style=color:#f92672>=</span>public-phy:NoSchedule
</span></span></code></pre></div><h2 id=0x03-测试>0x03 测试<a hidden class=anchor aria-hidden=true href=#0x03-测试>#</a></h2><ol><li>测试的Deployment如下</li></ol><pre tabindex=0><code>apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
</code></pre><ol start=2><li>验证提交到指定物理队列中的Pod默认增加NodeSelector和Toleration</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl apply -f nginx_deployment.yaml --namespace public
</span></span><span style=display:flex><span>$ kubectl describe pod nginx-deployment-574b87c764-kb9k7 --namespace public
</span></span><span style=display:flex><span>Node-Selectors:  node-restriction.kubernetes.io/physical_queue<span style=color:#f92672>=</span>public-phy
</span></span><span style=display:flex><span>Tolerations:     node.kubernetes.io/not-ready:NoExecute <span style=color:#66d9ef>for</span> 300s
</span></span><span style=display:flex><span>                 node.kubernetes.io/unreachable:NoExecute <span style=color:#66d9ef>for</span> 300s
</span></span><span style=display:flex><span>                 node-restriction.kubernetes.io/physical_queue<span style=color:#f92672>=</span>public-phy:NoSchedule
</span></span></code></pre></div><ol start=3><li>验证Pod是否都配置到一起</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl describe node kind-worker
</span></span></code></pre></div><ol start=4><li>验证和物理队列中指定的NodeSelector冲突的Pod无法提交</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>$ kubectl delete deployment nginx-deployment --namespace public
</span></span><span style=display:flex><span><span style=color:#75715e># 修改nginx_deployment.yaml ，增加spec.template.spec.nodeSelector</span>
</span></span><span style=display:flex><span>      nodeSelector:
</span></span><span style=display:flex><span>        node-restriction.kubernetes.io/physical_queue: second-phy
</span></span><span style=display:flex><span><span style=color:#75715e># 验证能否部署</span>
</span></span><span style=display:flex><span>$ kubectl apply -f nginx_deployment.yaml --namespace public
</span></span><span style=display:flex><span><span style=color:#75715e># 查看deployments</span>
</span></span><span style=display:flex><span>$ kubectl describe replicaset nginx-deployment-585fcd8d7d --namespace public
</span></span><span style=display:flex><span>  Warning  FailedCreate  49s <span style=color:#f92672>(</span>x15 over 2m11s<span style=color:#f92672>)</span>  replicaset-controller  Error creating: pods is forbidden: pod node label selector conflicts with its namespace node label selector
</span></span></code></pre></div><h2 id=0x04-相关问题>0x04 相关问题<a hidden class=anchor aria-hidden=true href=#0x04-相关问题>#</a></h2><h3 id=q-nodeselector无法使用set-based语法httpskubernetesiodocsconceptsoverviewworking-with-objectslabelsset-based-requirement导致逻辑队列namespace无法绑定多个物理队列>Q: NodeSelector无法使用<a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#set-based-requirement>Set-based语法</a>，导致逻辑队列（NameSpace）无法绑定多个物理队列<a hidden class=anchor aria-hidden=true href=#q-nodeselector无法使用set-based语法httpskubernetesiodocsconceptsoverviewworking-with-objectslabelsset-based-requirement导致逻辑队列namespace无法绑定多个物理队列>#</a></h3><p>后续考虑使用<a href=https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#node-affinity>Node Affinity</a>配置节点亲和性。但是目前并没有现成的<code>Adminssion Controller</code>去给Namespace绑定默认的节点亲和性，如有需求需要自己开发。</p><p>NodeSelector 和 Toleration 的功能，可以被 Node Affinity 进行替代，且后者提供更高级的调度功能，后续尝试是否基于此进行资源调度的整体设计。</p><p>此外Node Affinity还可以实现一个逻辑队列绑定多个物理队列的情况下，配置物理队列的调度权重的功能，即优先部署到某个物理队列。</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://ninehills.tech/tags/blog/>blog</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://ninehills.tech/>九原山</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>